import streamlit as st
import os
# On importe tout le n√©cessaire depuis votre pipeline
from rag_pipeline import initialize_rag_pipeline, MANUAL_PROMPT_TEMPLATE, PDF_FOLDER_PATH

# --- Configuration de la Page Streamlit ---
st.set_page_config(
    page_title="SmartPDF - Gemini 3 RAG",
    page_icon="üìÑ",
    layout="wide"
)

# --- Barre Lat√©rale : Gestion des Documents ---
with st.sidebar:
    st.title("üìÅ Gestion des PDF")
    st.write("Ajoutez vos documents pour alimenter l'IA.")

    # 1. Cr√©ation du dossier PDF s'il n'existe pas
    if not os.path.exists(PDF_FOLDER_PATH):
        os.makedirs(PDF_FOLDER_PATH)

    # 2. Zone d'upload
    uploaded_files = st.file_uploader(
        "D√©posez vos PDF ici", 
        type="pdf", 
        accept_multiple_files=True
    )

    # 3. Traitement des fichiers upload√©s
    if uploaded_files:
        files_saved = False
        for uploaded_file in uploaded_files:
            file_path = os.path.join(PDF_FOLDER_PATH, uploaded_file.name)
            # On √©crit le fichier s'il n'existe pas encore
            if not os.path.exists(file_path):
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                files_saved = True
        
        if files_saved:
            st.success("Nouveaux documents d√©tect√©s !")

        # --- Bouton de r√©-indexation avec retour visuel ---
        if st.button("üîÑ Lancer la r√©-indexation"):
            with st.status("Mise √† jour de la base de connaissances...", expanded=True) as status:
                st.write("Nettoyage du cache syst√®me...")
                st.cache_resource.clear()
                
                st.write("Analyse des PDF et cr√©ation des embeddings...")
                # On force l'initialisation pour reconstruire l'index FAISS
                llm, retriever = initialize_rag_pipeline()
                
                status.update(label="Indexation termin√©e avec succ√®s !", state="complete", expanded=False)
            
            st.toast("L'IA est √† jour !", icon="‚úÖ")
            st.rerun()

    st.divider()
    st.caption("Propuls√© par Gemini 3 Flash & FAISS")

# --- Corps Principal ---
st.title("ü§ñ Chatbot pour vos PDF üìÑ")
st.caption("Posez des questions sur vos documents en temps r√©el.")

# --- Initialisation du Pipeline RAG ---
try:
    # Cette fonction est cach√©e, elle ne recalculera rien sauf si on a vid√© le cache
    llm, retriever = initialize_rag_pipeline()
except Exception as e:
    st.error(f"Erreur lors du d√©marrage : {e}")
    llm, retriever = None, None

# Gestion du cas o√π aucun document n'est pr√©sent
if not llm or not retriever:
    st.info("üëã **Bienvenue !** Pour commencer, veuillez ajouter un ou plusieurs fichiers PDF dans la barre lat√©rale √† gauche.")
    st.stop()

# --- Initialisation de l'historique du Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant", 
        "content": "Bonjour ! J'ai analys√© vos documents. Comment puis-je vous aider ?"
    }]

# Affichage de l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Logique de Question/R√©ponse ---
if prompt := st.chat_input("Posez votre question ici..."):
    
    # 1. Message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. R√©ponse assistant
    with st.chat_message("assistant"):
        with st.spinner("Recherche et r√©flexion..."):
            try:
                # Recherche des documents pertinents
                retrieved_docs = retriever.invoke(prompt)
                
                # Formatage du contexte
                context_string = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                
                # Pr√©paration du prompt final
                final_prompt = MANUAL_PROMPT_TEMPLATE.format(
                    context=context_string,
                    question=prompt
                )
                
                # Appel √† Gemini 3
                response = llm.invoke(final_prompt)

                # Extraction du texte (Gestion sp√©cifique Gemini 3)
                if isinstance(response.content, list):
                    answer = response.content[0].get('text', '')
                else:
                    answer = response.content

                # Affichage du r√©sultat
                st.markdown(answer)
                
                # Affichage des sources
                if retrieved_docs:
                    with st.expander("üîç Voir les sources consult√©es"):
                        for i, doc in enumerate(retrieved_docs):
                            source_file = doc.metadata.get('source', 'Inconnue')
                            source_page = doc.metadata.get('page', 'Inconnue')
                            # Nettoyage du nom de fichier pour l'affichage
                            file_name = os.path.basename(source_file)
                            st.write(f"**Source {i+1} :** {file_name} (Page {source_page+1})")
                            st.caption(f'"{doc.page_content[:200]}..."')

                # Sauvegarde dans l'historique
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"Une erreur est survenue : {e}")