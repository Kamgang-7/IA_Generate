import os

import streamlit as st

# =========================================================
# 1. INTEGRATION LANGFUSE (Monitoring & Tracing)
# =========================================================
# On tente d'importer le CallbackHandler pour LangChain.
# Si le module n'est pas install√©, l'app ne crash pas gr√¢ce au try/except.
try:
    from langfuse.langchain import CallbackHandler

    HAS_LANGFUSE = True
except Exception:
    HAS_LANGFUSE = False

from rag_pipeline import (
    MANUAL_PROMPT_TEMPLATE,
    PDF_FOLDER_PATH,
    initialize_rag_pipeline,
)

# Configuration de l'interface Streamlit (titre de l'onglet, ic√¥ne, mode large
st.set_page_config(page_title="SmartPDF - Assistant RAG Intelligent", page_icon="üöÄ", layout="wide")

# =========================================================
# 2. TEMPLATES DE PROMPTS
# =========================================================
# Ce prompt sert √† "contextualiser" la question.
REWRITE_PROMPT = """
Sur la base de l'historique de la conversation et de la derni√®re
question de l'utilisateur, reformule une question autonome qui 
peut √™tre comprise sans l'historique. 
Cette question servira √† faire une recherche dans des documents PDF.

Historique :
{history}

Derni√®re question : {question}

Question reformul√©e (sois pr√©cis et direct) :
"""

# =========================================================
# 3. CONFIGURATION LANGFUSE & CALLBACKS
# =========================================================
pk = os.getenv("LANGFUSE_PUBLIC_KEY")
sk = os.getenv("LANGFUSE_SECRET_KEY")

langfuse_handler = None
if HAS_LANGFUSE and pk and sk:
    langfuse_handler = CallbackHandler()
    st.sidebar.success("‚úÖ Langfuse activ√© (LLM tracing)")
else:
    st.sidebar.warning("‚ÑπÔ∏è Langfuse d√©sactiv√© (keys absentes ou module manquant)")

st.sidebar.divider()

# Le dictionnaire 'config' sera pass√© aux appels LLM pour envoyer les logs √† Langfuse
config = {"callbacks": [langfuse_handler]} if langfuse_handler else {}

# =========================================================
# 4. GESTION DE L'ETAT (Session State)
# =========================================================
# Streamlit recharge tout le script √† chaque interaction.
# On utilise st.session_state pour garder les donn√©es en m√©moire.

# Initialisation de l'historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Flag pour savoir si l'index BM25 doit √™tre reconstruit (ex: apr√®s un upload)
if "index_needed" not in st.session_state:
    st.session_state.index_needed = False


# =========================================================
# 5. FONCTIONS D'AIDE (Helpers)
# =========================================================
def _extract_text(content):
    """Nettoie la sortie du LLM pour s'assurer qu'on r√©cup√®re bien une cha√Æne de caract√®res."""
    if isinstance(content, list) and len(content) > 0:
        first = content[0]
        if isinstance(first, dict):
            return first.get("text", str(first))
        return str(first)
    return str(content)


def build_index_now():
    """D√©clenche la cr√©ation de l'index BM25 √† partir des fichiers PDF du dossier source."""
    st.cache_resource.clear()
    initialize_rag_pipeline(force_reindex=True)
    st.session_state.index_needed = False


# =========================================================
# 6. BARRE LAT√âRALE (Sidebar)
# =========================================================
with st.sidebar:
    st.header("1) Charger des documents PDF")
    os.makedirs(PDF_FOLDER_PATH, exist_ok=True)  # Cr√©e le dossier PDF s'il n'existe pas

    # Widget de t√©l√©chargement multiple
    uploaded_files = st.file_uploader("Chargez un ou plusieurs PDFs", type="pdf", accept_multiple_files=True)

    # Sauvegarde physique des fichiers sur le serveur/PC
    if uploaded_files:
        files_saved = False
        for f in uploaded_files:
            file_path = os.path.join(PDF_FOLDER_PATH, f.name)
            if not os.path.exists(file_path):
                with open(file_path, "wb") as file:
                    file.write(f.getbuffer())
                files_saved = True

        if files_saved:
            st.session_state.index_needed = True
            st.session_state.last_upload_msg = f"{len(uploaded_files)} fichier(s) pr√™t(s) √† l'indexation."

    # Affichage persistant du message de succ√®s
    if st.session_state.index_needed and "last_upload_msg" in st.session_state:
        st.success(st.session_state.last_upload_msg)
        st.warning("‚ö†Ô∏è Cliquez sur le bouton ci-dessous pour mettre √† jour l'IA.")

    st.divider()

    st.header("2) Recr√©er l'index")
    if st.button("üîÑ Re-g√©n√©rer l'index", use_container_width=True):
        with st.spinner("Indexation en cours..."):
            build_index_now()
            # On nettoie le message apr√®s indexation
            if "last_upload_msg" in st.session_state:
                del st.session_state.last_upload_msg
        st.success("‚úÖ Index mis √† jour !")
        st.rerun()

    st.divider()

    # Information p√©dagogique sur les scores de pertinence
    st.info(
        """
        **üí° Score BM25 (Confiance) :**
        * **> 10** : Tr√®s pertinent ‚úÖ
        * **< 2** : Peu pr√©cis / Al√©atoire ‚ö†Ô∏è
        
        *Plus le score est √©lev√©, plus la source est fiable.*
        """
    )

# =========================================================
# 7. CORPS DE L'APPLICATION
# =========================================================
st.title("SmartPDF - Assistant RAG Intelligent ü§ñ")

# Guide rapide pour l'utilisateur
with st.expander("Guide de d√©marrage rapide", expanded=True):
    st.markdown(
        """
    Bienvenue sur **SmartPDF** ! Pour poser des questions √† vos documents, suivez ces √©tapes :
    1.  **Charger vos documents** : Utilisez le bouton dans la barre lat√©rale pour uploader vos PDF.
    2.  **Indexer les fichiers** : Cliquez sur **'Re-g√©n√©rer l'index'**.
    3.  **Discutez** : Posez votre question dans la barre de chat en bas de l'√©cran. 
    
    *Note : Si vous oubliez d'indexer, le syst√®me le fera automatiquement lors de votre premi√®re question.*
    """
    )

# Initialisation silencieuse du pipeline (charge l'index existant si disponible)
llm, retriever = initialize_rag_pipeline()

# Affichage de tous les messages pr√©c√©dents (historique de session)
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Zone de saisie utilisateur
prompt = st.chat_input("Votre question (vous pouvez √©crire m√™me si l'index n'est pas encore pr√™t).")

if prompt:
    # On ajoute la question de l'utilisateur √† l'historique et on l'affiche
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # R√©ponse de l'assistant
    with st.chat_message("assistant"):

        # --- ETAPE 1 : Verification de l'Index ---
        # 1) Si aucun index n'existe ou si de nouveaux fichiers attendent, on indexe avant de r√©pondre
        if (not callable(retriever)) or st.session_state.index_needed:
            with st.spinner("üì¶ Construction de l‚Äôindex BM25 (une seule fois)..."):
                build_index_now()
                # Recharger pipeline apr√®s indexation
                llm, retriever = initialize_rag_pipeline()

        # 2) S√©curit√© si le chargement √©choue
        if not llm or not callable(retriever):
            st.error("Pipeline non pr√™t (v√©rifie tes PDFs / GOOGLE_API_KEY).")
            st.stop()

        # --- ETAPE 2 : Reformulation (Query Rewriting) ---
        search_query = prompt
        history_str = ""
        # Si on a d√©j√† discut√©, on demande au LLM de cr√©er une question autonome
        if len(st.session_state.messages) > 1:
            past_messages = st.session_state.messages[-4:-1]
            history_str = "\n".join([f"{m['role']}: {m['content']}" for m in past_messages])

            rewrite_input = REWRITE_PROMPT.format(history=history_str, question=prompt)
            rewrite_res = llm.invoke(rewrite_input, config=config)
            search_query = _extract_text(rewrite_res.content)

        st.caption(f"üîç **Requ√™te optimis√©e :** *{search_query}*")

        # --- ETAPE 3 : Recherche (Retrieval) ---
        with st.spinner("Recherche dans les documents..."):
            hits = retriever(search_query, k=4)
            doc_context = "\n---\n".join([h["text"] for h in hits])

        # --- ETAPE 4 : G√©n√©ration de la r√©ponse (Generation) ---
        with st.spinner("R√©daction de la r√©ponse..."):
            combined_context = f"[HISTORIQUE]\n{history_str}\n\n[DOCUMENTS]\n{doc_context}"
            final_prompt = MANUAL_PROMPT_TEMPLATE.format(context=combined_context, question=prompt)

            # Appel final au LLM (Gemini)
            response = llm.invoke(final_prompt, config=config)
            answer = _extract_text(response.content)

            st.markdown(answer)
            # Sauvegarde de la r√©ponse dans l'historique
            st.session_state.messages.append({"role": "assistant", "content": answer})

        # --- ETAPE 5 : Affichage des Sources ---
        if hits:
            with st.expander("üîç Sources consult√©es & Pertinence"):
                for h in hits:
                    score = h.get("score", 0)
                    source_name = os.path.basename(h["meta"]["source"])
                    page_num = h["meta"]["page"] + 1

                    st.write(f"üìÑ **{source_name}** (Page {page_num})")
                    st.code(f"Score BM25 : {score:.2f}", language="markdown")
                    st.text(f"‚Äú{h['text'][:200]}‚Ä¶‚Äù")
                    st.divider()
