import streamlit as st
from rag_pipeline import initialize_rag_pipeline, MANUAL_PROMPT_TEMPLATE

# --- Configuration de la Page Streamlit ---
st.set_page_config(
    page_title="Chat avec vos PDF",
    page_icon="üìÑ",
    layout="centered"
)

st.title("Chatbot pour vos PDF üìÑ")
st.caption("Posez des questions sur n'importe quel document de votre dossier 'PDF'")

# --- Initialisation du Pipeline RAG ---
# Cela utilise le cache : le code dans `initialize_rag_pipeline` 
# ne s'ex√©cute qu'une fois.
try:
    llm, retriever = initialize_rag_pipeline()
except Exception as e:
    st.error(f"Une erreur est survenue lors du d√©marrage : {e}")
    llm, retriever = None, None

# Si l'initialisation √©choue, on arr√™te l'application
if not llm or not retriever:
    st.warning("Le pipeline RAG n'a pas pu √™tre initialis√©. V√©rifiez les erreurs ci-dessus et votre fichier .env.")
    st.stop()

# --- Initialisation de l'historique du Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant", 
        "content": "Bonjour ! Je suis pr√™t √† r√©pondre √† vos questions sur les documents du dossier PDF."
    }]

# Afficher les messages de l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Logique du Chat ---
if prompt := st.chat_input("Posez votre question ici..."):
    
    # 1. Ajouter le message de l'utilisateur √† l'historique et l'afficher
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Pr√©parer et afficher la r√©ponse de l'assistant
    with st.chat_message("assistant"):
        with st.spinner("Recherche dans les documents..."):
            try:
                # 3. R√©cup√©rer les documents pertinents (le "R" de RAG)
                retrieved_docs = retriever.invoke(prompt)
                
                # 4. Formater le contexte
                context_string = "\n---\n".join([doc.page_content for doc in retrieved_docs])
                
                # 5. Formater le prompt final
                final_prompt = MANUAL_PROMPT_TEMPLATE.format(
                    context=context_string,
                    question=prompt
                )
                
                # --- 6. G√©n√©rer la r√©ponse (le "G" de RAG) ---
                st.spinner("G√©n√©ration de la r√©ponse...")
                response = llm.invoke(final_prompt)

                # --- NOUVELLE LOGIQUE D'EXTRACTION ---
                # On v√©rifie si le contenu est une liste (cas du mod√®le Gemini 3 Preview) 
                # ou une simple cha√Æne de caract√®res.
                if isinstance(response.content, list):
                    # On extrait le texte du premier √©l√©ment de la liste
                    answer = response.content[0].get('text', '')
                else:
                    # Cas classique
                    answer = response.content

                # On ignore volontairement les 'extras' ou les signatures pour l'affichage
                # ---------------------------------------

                # 7. Afficher la r√©ponse
                st.markdown(answer)
                
                # 8. (Am√©lioration) Afficher les sources utilis√©es
                with st.expander("Afficher les sources"):
                    for i, doc in enumerate(retrieved_docs):
                        source_file = doc.metadata.get('source', 'Inconnue')
                        source_page = doc.metadata.get('page', 'Inconnue')
                        st.write(f"**Source {i+1} (Fichier: {source_file}, Page: {source_page+1})**")
                        st.caption(f'"{doc.page_content[:250]}..."')

                # 9. Ajouter la r√©ponse de l'assistant √† l'historique
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"Une erreur est survenue lors de la g√©n√©ration de la r√©ponse : {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"D√©sol√©, une erreur est survenue: {e}"})