---
title: SmartPDF-RAG
emoji: ğŸ“„
colorFrom: blue
colorTo: green
sdk: streamlit
app_file: chatbot_app.py
pinned: false
---

# ğŸ“„ SmartPDF-RAG : Assistant Intelligent avec Gemini 3 & BM25

![Python](https://img.shields.io/badge/python-3.11+-blue)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B)
![Docker](https://img.shields.io/badge/Docker-2496ED)

Ce projet est une application de RAG (Retrieval-Augmented Generation) permettant d'interroger des documents PDF de maniÃ¨re naturelle. Il utilise la puissance de Google Gemini 3 combinÃ©e Ã  un moteur de recherche BM25 pour garantir des rÃ©ponses prÃ©cises et sourcÃ©es.

## ğŸ“Œ Sommaire
1. [ğŸš€ Concept](#-concept)
2. [âœ¨ Points forts](#-points-forts)
3. [ğŸ› ï¸ Choix techniques](#ï¸-choix-techniques)
4. [âš ï¸ Limitations](#ï¸-limitations)
5. [âš™ï¸ Installation et lancement](#ï¸-installation-et-lancement)
6. [ğŸŒ DÃ©ploiement sur Hugging Face Spaces](#ï¸-DÃ©ploiement-sur-hugging-face-spaces)
7. [ğŸ“‚ Utilisation](#-utilisation)
8. [ğŸ“ Structure du projet](#-structure-du-projet)
9. [ğŸ’¡ Fonctionnement de l'indexation](#-fonctionnement-de-lindexation)
10. [Perspectives d'Ã©volution](#-perspectives-dÃ©volution)

## ğŸš€ Concept
L'application permet d'uploader des documents PDF et de discuter avec eux via une interface de chat. Contrairement Ã  un chatbot classique, celui-ci "lit" vos documents en temps rÃ©el pour extraire les passages pertinents avant de gÃ©nÃ©rer une rÃ©ponse, Ã©vitant ainsi les hallucinations et garantissant la vÃ©racitÃ© des informations.

## âœ¨ Points forts
- **RÃ©ponses SourcÃ©es** : Affichage automatique des extraits de PDF utilisÃ©s pour chaque rÃ©ponse.
- **Vitesse & FiabilitÃ©** : Utilisation de `uv` pour des builds ultra-rapides.
- **Architecture Propre** : Code lintÃ© (Ruff & Black) et conteneurisÃ© pour un dÃ©ploiement sans erreurs.
- **Monitoring IntÃ©grÃ©** : Suivi des traces et de la latence via Langfuse cloud.

## ğŸ› ï¸ Choix techniques
Nous avons privilÃ©giÃ© des outils offrant un compromis optimal entre simplicitÃ© et performance :
- **LLM (IA)** : `gemini-3-flash-preview` pour sa grande fenÃªtre de contexte et son faible coÃ»t.
- **Moteur de recherche (BM25)** : Choisi Ã  la place d'une base de donnÃ©es vectorielle pour sa prÃ©cision sur les termes techniques exacts et son absence de coÃ»t d'embedding.
- **Orchestration** : **LangChain** pour la gestion fluide de la mÃ©moire et du flux RAG.
- **Conteneurisation** : **Docker & Docker Compose** pour garantir un environnement d'exÃ©cution identique sur toutes les machines.

## âš ï¸ Limitations
- **Format** : Seuls les fichiers `.pdf` sont acceptÃ©s pour le moment.
- **SÃ©mantique** : Le moteur BM25 se base sur les mots-clÃ©s ; il peut Ãªtre moins performant qu'un moteur vectoriel sur des questions purement conceptuelles sans termes communs.
- **Stockage** : L'index est stockÃ© localement (`bm25_index/`) et n'est pas persistant sur une base de donnÃ©es cloud.

## ğŸš€ Installation et lancement

Ce projet utilise [uv](https://github.com/astral-sh/uv) pour une gestion simplifiÃ©e.

1. PrÃ©requis
CrÃ©ez un fichier .env Ã  la racine du projet :
```bash
GOOGLE_API_KEY="VOTRE_CLE_API_GOOGLE"

# Optionnel (Monitoring)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=
LANGFUSE_HOST="https://cloud.langfuse.com"
```

### 2. Lancement avec Docker (recommandÃ©)
```bash
docker-compose up --build
```
L'application sera disponible sur http://localhost:8501.

### 3. Installation locale avec uv
Si vous prÃ©fÃ©rez lancer le projet nativement :
```bash
uv sync
uv run streamlit run chatbot_app.py
```

## ğŸŒ DÃ©ploiement sur Hugging Face Spaces

Ce projet est compatible avec Hugging Face Spaces (SDK Docker).

1. SDK : Streamlit
2. Port : L'application utilise par dÃ©faut le port 8501, mais peut Ãªtre configurÃ©e sur 7860 pour HF dans le Dockerfile.
3. Secrets : Ajoutez votre GOOGLE_API_KEY dans les Settings > Variables and secrets de votre Space Hugging Face.

## ğŸ“‚ Utilisation

1. Placez vos fichiers PDF dans le dossier `PDF/`.
2. Lancez l'application via uv :
```bash
uv run streamlit run chatbot_app.py
```
3. Posez vos questions ! L'application crÃ©era automatiquement un dossier `bm25_index/` pour stocker les donnÃ©es traitÃ©es.

## ğŸ“ Structure du projet
```Plaintext
.
â”œâ”€â”€ PDF/                 # Dossier source des documents PDF
â”œâ”€â”€ bm25_index/          # Stockage local de l'index BM25 (manifeste + store)
â”œâ”€â”€ chatbot_app.py       # Interface Streamlit et logique de conversation
â”œâ”€â”€ rag_pipeline.py      # CÅ“ur du pipeline (BM25, Tokenization, LLM)
â”œâ”€â”€ Dockerfile           # Configuration de l'image Docker
â”œâ”€â”€ docker-compose.yml   # Orchestration des services app et lint
â””â”€â”€ pyproject.toml       # DÃ©pendances et configuration des outils (Ruff, Black)
```

## ğŸ’¡ Fonctionnement de l'indexation

L'application surveille automatiquement le dossier PDF/. Un "fingerprint" (empreinte numÃ©rique MD5) est calculÃ© Ã  chaque lancement :

- Un calcul est fait sur l'ensemble des fichiers du dossier `PDF/`.
- Si le fingerprint change (ajout/suppression), l'index se reconstruit automatiquement.
- Sinon, l'index est chargÃ© instantanÃ©ment depuis le dossier `bm25_index/`.

## Perspective d'Ã©volution
- IntÃ©gration d'un mode hybride (BM25 + VectorDB type FAISS).
- Support des fichiers Word/Markdown.
- Gestion de l'OCR pour les PDF scannÃ©s.
- intÃ©gration d'un dashboard de coÃ»t en temps rÃ©el (via Langfuse API).